{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac408b5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7619294def77410a539e618e2428f0d5",
     "grade": false,
     "grade_id": "cell-b00828259c8e42e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# RO47019: Intelligent Control Systems Practical Assignment\n",
    "* Period: 2023-2024, Q3\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/500969\n",
    "* Instructor: Cosimo Della Santina (C.DellaSantina@tudelft.nl)\n",
    "* Teaching assistant: Maria de Neves de Fonseca (M.deNevesdeFonseca-1@student.tudelft.nl)\n",
    "* (c) TU Delft, 2024\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Remove `raise NotImplementedError()` afterwards. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in your names, student numbers, netID, and emails below.\n",
    "STUDENT_1_NAME = \"Timothy van den Heuvel\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"5606403\"\n",
    "STUDENT_1_NETID = \"timvandenheuve\"\n",
    "STUDENT_1_EMAIL = \"t.j.vandenheuvel-1@student.tudelft.nl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba32571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "042927213b84aa368aa3ea72caa4cb60",
     "grade": true,
     "grade_id": "cell-9f148ec62e0de49c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert STUDENT_1_NAME != \"\"\n",
    "assert STUDENT_1_STUDENT_NUMBER != \"\"\n",
    "assert STUDENT_1_NETID != \"\"\n",
    "assert STUDENT_1_EMAIL != \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af317a94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95c5b11f9ac3896252d342cabb38d867",
     "grade": false,
     "grade_id": "cell-4ea391677951116c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions (also after the course is finished), and do *not* copy solutions from others. By submitting your solutions, you claim that you alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled Q&A hours to ask the TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for you that you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c956945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32ecec16-2871-4af9-9c97-559a00f4eecc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "172f3ea78874b48ece316505ebe38f98",
     "grade": false,
     "grade_id": "cell-f5e628962eac03a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Task 1b - Vision-based angle prediction (20 points)\n",
    "\n",
    "**Authors:** Chuhan Zhang (C.Zhang-8@tudelft.nl), Tomás Coleman, Maximilian Stölzle (M.W.Stolzle@tudelft.nl)\n",
    "\n",
    "**Warning** To help you, we have marked parts of the code where we want you to contribute. These parts are often marked with a start comment, `Task 1b.i.j: ...` and a closing comment, `Task 1b.i.j: END`. You are free to add code outside of the designated areas. However, we cannot guarantee that this won't affect the intended behavior of the code.\n",
    "\n",
    "In this file, we will implement two Convolutional Neural Network (CNN) models to predict the position of the single pendulum based on input image data. Each CNN model will try a separate way to predict angles: direct or indirect. Direct prediction means that the network outputs a prediction of the link angle $\\hat \\theta$, and then updates the network parameters by comparing the loss between the predicted $\\hat \\theta$ and the label $\\theta$. However, indirect prediction means that the dataset has changed. Label is no longer a single angle, but an array containing the $\\sin \\theta$ value and $\\cos \\theta$ value of the angle. The output of the network is no longer a single $\\hat \\theta$, but the predicted $[\\hat {\\sin \\theta}, \\hat {\\cos \\theta}]$. The network updates parameters by comparing the loss between $[\\sin \\theta, \\cos \\theta]$ and $[\\hat {\\sin \\theta}, \\hat {\\cos \\theta}]$.\n",
    "\n",
    "The following cells import all the necessary packages and external functions to run the code properly. First, we want to check whether the images are generated properly and create the directory for output plots and models. Different dataset classes are also created from the information gathered in notebook 1a. Finally, different Pytorch data loaders will be created for each network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "badaa419-10cc-4700-8bcb-643be394f636",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "128221cdc7cb27bafa9f1571447b2313",
     "grade": false,
     "grade_id": "cell-95396aef1ca5b8b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from tqdm.notebook import tqdm  # progress bar\n",
    "from typing import List, Tuple\n",
    "from distutils.util import strtobool\n",
    "\n",
    "# define boolean to check if the notebook is run for the purposes of autograding\n",
    "AUTOGRADING = strtobool(os.environ.get(\"AUTOGRADING\", \"false\"))\n",
    "\n",
    "\n",
    "# seed the random number generators\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "def manual_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set manual seeds\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed to sett.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "class DirectoryNotFoundException(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised when a specified directory is not found.\n",
    "\n",
    "    This exception is used to signal that an operation which expects\n",
    "    a certain directory to exist has failed because the directory\n",
    "    was not found in the expected location. It extends the standard\n",
    "    Exception class and can be used to provide more specific error\n",
    "    handling that is distinct from general exceptions.\n",
    "\n",
    "    Attributes:\n",
    "        None - Inherits all attributes and methods from the base Exception class.\n",
    "\n",
    "    Methods:\n",
    "        None - Inherits all methods from the base Exception class and does not\n",
    "        define any additional behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "class ModelTypeNotFoundException(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised when a specified model type is not found.\n",
    "\n",
    "    This exception is used in contexts where a specific type of\n",
    "    model is expected to be present or accessible, but it cannot\n",
    "    be located or identified. The exception is particularly useful\n",
    "    in scenarios involving machine learning or data processing\n",
    "    pipelines where the absence of a required model type can lead\n",
    "    to failure of the process.\n",
    "\n",
    "    Attributes:\n",
    "        None - Inherits all attributes and methods from the base Exception class.\n",
    "\n",
    "    Methods:\n",
    "        None - Inherits all methods from the base Exception class and does not\n",
    "        define any additional behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# check the directory for datasets\n",
    "DATASET_DIR = Path(\"datasets\") / \"pendulum_dataset\"\n",
    "if not DATASET_DIR.exists():\n",
    "    raise DirectoryNotFoundException(\n",
    "        f\"The pendulum dataset doesn't exist yet. Please run all cells in the `task_1a_extract_dataset` notebook.\"\n",
    "    )\n",
    "\n",
    "# create the directory for well-trained neural network models\n",
    "STATEDICTS_DIR = Path(\"statedicts\")\n",
    "STATEDICTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598ae47-d4af-4e75-a3c4-4e39d08f487e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa73308a91534e8f8f090c9eb8f660e4",
     "grade": false,
     "grade_id": "cell-4311cd7e06af6fe9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then, we would like to prepare the training and test sets for the CNNs. In this assignment, we use two ways to predict the state: direct and indirect. Therefore, we prepare the datasets with two formats of labels, called $theta$ and $trig$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb729a2-3692-402d-9074-356418a100ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cbf58f92c8c2430fe402198e2544087",
     "grade": false,
     "grade_id": "cell-9a4d22c564a03885",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CNNDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset class for a CNN, designed to load images and corresponding labels.\n",
    "\n",
    "    This class extends the PyTorch Dataset class, making it suitable for use with\n",
    "    PyTorch data loaders and other utilities. It is designed to load image data\n",
    "    and their associated labels from a specified directory, performing optional\n",
    "    transformations on the images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path: PathLike, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset object with the data path, optional transform, and dataset length.\n",
    "\n",
    "        Args:\n",
    "            dataset_path: Path to the directory where image and label files are stored.\n",
    "            transform: A function/transform that takes in an image and returns a transformed version.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        # determine the total dataset length\n",
    "        i = 0\n",
    "        while (self.dataset_path / f\"image{i}.npz\").exists():\n",
    "            i += 1\n",
    "        self.len = i\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves an image and its corresponding label at the given index `idx`. The image is\n",
    "        loaded from a .npz file, optionally transformed, and then returned along with its label\n",
    "        (both in position form and as sine and cosine components).\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the image and label to be loaded.\n",
    "        Returns:\n",
    "            x: The image as a torch.Tensor, possibly transformed.\n",
    "            label_theta: The position label as a torch.FloatTensor\n",
    "            label_trig: A torch.FloatTensor with two elements, representing the sine and cosine\n",
    "               components of the label.\n",
    "        \"\"\"\n",
    "        x = np.array(np.load(self.dataset_path / f\"image{idx}.npz\")[\"arr_0\"])\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        theta = np.load(self.dataset_path / f\"label{idx}.npz\")[\"arr_0\"]\n",
    "        sin = np.sin(theta)\n",
    "        cos = np.cos(theta)\n",
    "\n",
    "        theta = torch.tensor(np.array(theta)).type(torch.FloatTensor)\n",
    "        trig = torch.tensor(np.array([sin, cos])).type(torch.FloatTensor)\n",
    "        return x, theta, trig\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total number of samples available in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            self.len: The total number of images (or samples) in the dataset.\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b46702-f5db-49c1-b867-283c163c99f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a282dcdc7c4cfff2e4adc043895c973",
     "grade": false,
     "grade_id": "cell-2795060e8922d903",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The size of the original image data is `500x500x3`, and we want to compress it into a smaller size to save the computation time. Therefore, we add _transforms_ for preprocessing data when building our `Dataset` object. We first use the `ToTensor()` method in PyTorch to convert pixel data distributed between `[0, 255]` into floating point tensor data distributed between `[0, 1]`. Then, we use `Grayscale()` to convert the RGB images into grayscale images. _**Optional question**: Why can we convert the input color image into a grayscale image in this task? Can this be done for all tasks?_ Then we use `CenterCrop()` to extract the center `240x240`px area of the original image, and use `Resize()` to downsample the image to `24x24`px. With this preprocessing pipeline, we obtained training data with each input sample being a tensor of dimension `24x24x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79320e4f-fa96-471a-a0fb-530542fffc5d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b14674db259b6dd1ad944cee27e15d",
     "grade": false,
     "grade_id": "cell-f72b9c17c60bb8b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.CenterCrop(240),\n",
    "        transforms.Resize((24, 24)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ca7d0-d7bd-496a-a99c-4530c04b6e85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "136aa5f8849fc62f325c486f5e1db1e4",
     "grade": false,
     "grade_id": "cell-4dc1782a7fb39615",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is worth noting that, this `DataLoader` returns three variables: the image $x$, the corresponding label $\\theta$, and $[\\sin \\theta, \\cos \\theta]$ data. The `DataLoader` is consistent for direct and indirect prediction tasks. You must be careful about using underscores in your code to ignore unnecessary return values for the task.\n",
    "\n",
    "In this task, we divide the entire data set into three parts: training set, validation set, and test set. The split ratio between training, validation, and testing is 50%, 20%, and 30%. By calling the `load_dataloaders` method, the data loaders for training, validation, and testing are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb5d854-74c7-4c8b-9bbf-e6a609165a27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bff80a2e0217ae53d43b9475f1f2f0ff",
     "grade": false,
     "grade_id": "cell-a08f7e2a8ff6aed9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 3600 samples\n"
     ]
    }
   ],
   "source": [
    "def load_dataloaders(\n",
    "    dataset_path: PathLike,\n",
    "    val_ratio: float = 0.2,\n",
    "    test_ratio: float = 0.3,\n",
    "    batch_size: int = 32,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads and returns data loaders for training, validation, and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the directory where the dataset is stored.\n",
    "        val_ratio: The ratio of the dataset to be used for validation.\n",
    "                    It must be between 0 and 1. Defaults to 0.2.\n",
    "        test_ratio: The ratio of the dataset to be used for testing.\n",
    "                    It must be between 0 and 1. Defaults to 0.3.\n",
    "        batch_size: The number of samples per batch to load. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        train_dataloader: The DataLoader instances for the training.\n",
    "        val_dataloader: The DataLoader instances for the validation.\n",
    "        test_dataloader: The DataLoader instances for the testing.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the sum of val_ratio and test_ratio is larger than 1.0\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        0.0 <= val_ratio <= 1.0\n",
    "    ), \"Validation ratio needs to be in the interval [0, 1].\"\n",
    "    assert 0.0 <= test_ratio <= 1.0, \"Test ratio needs to be in the interval [0, 1].\"\n",
    "    assert (\n",
    "        val_ratio + test_ratio\n",
    "    ) <= 1.0, \"The sum of val and test ratio needs to be in the interval [0, 1].\"\n",
    "\n",
    "    dataset = CNNDataset(dataset_path, transform=IMAGE_TRANSFORM)\n",
    "    print(f\"The dataset contains {len(dataset)} samples\")\n",
    "\n",
    "    train_size = int(len(dataset) * (1.0 - val_ratio - test_ratio))\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    test_size = int(len(dataset) * test_ratio)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = load_dataloaders(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb41372-69cb-4371-b8a8-fd01bcdb88db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59abd5959bf99e47189c2a1466261533",
     "grade": false,
     "grade_id": "cell-ec8e147966501c31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We show the data after preprocessing as below. Do you still remember the colorful, high-resolution dataset samples in task 1a? Now, they have become a low-resolution grayscale image. Please feel free to visualize multiple samples by changing the respective `sample_idx` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b268fa-f91b-4320-a7e3-e196a2057d69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a6c11086460c074ca3937da6a0f95e6",
     "grade": false,
     "grade_id": "cell-0ec17e0cef5ed98a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_dataset_sample(dataset_path: str, sample_idx: int = 1200) -> None:\n",
    "    \"\"\"\n",
    "    Displays a visual representation of an example image from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the directory where the dataset is stored.\n",
    "        sample_idx: The index of the sample to be visualized. Defaults to 1200.\n",
    "\n",
    "    Returns:\n",
    "        None: This function only displays an image and does not return any value.\n",
    "    \"\"\"\n",
    "    dataset = CNNDataset(dataset_path, transform=IMAGE_TRANSFORM)\n",
    "    img_example, _, _ = dataset[sample_idx]\n",
    "    img_example = img_example.permute(1, 2, 0)\n",
    "    plt.imshow(img_example, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_dataset_sample(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8408af6-f43a-4c87-9c39-81ce8cd7451b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d8a2674cb2d7a747f0b624d65082b81",
     "grade": false,
     "grade_id": "cell-bb32bd4a5f16ab74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We already have the function to build training, validation, and testing dataloaders. We will package the evaluation function in the following code block.\n",
    "\n",
    "Here, during the process of training, we adopt Mean Square Error (MSE) as the loss by invoking `nn.MSELoss()` from PyTorch. During the evalutation process (also in `evaluate_model` method), we adopt Mean Absolute Error (MAE) as the error by invoking `nn.L1Loss()`. _**Optional question:** why do we use MSE in the training process instead of MAE?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac377ae6-7d7d-4af7-bc20-72d4168da9a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a90f6f201ea16a81bc323d3e3303873a",
     "grade": false,
     "grade_id": "cell-393b7b53419f6731",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "error_fn = nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module, eval_loader: DataLoader, model_type: str = \"theta\"\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a theta model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to be evaluated.\n",
    "        eval_loader: The DataLoader for the evaluation dataset.\n",
    "        model_type: The type of model to be evaluated ('theta' or 'trig').\n",
    "                                    Defaults to 'theta'.\n",
    "\n",
    "    Returns:\n",
    "        loss: The mean loss of the model on the valuation dataset.\n",
    "        error: The mean error of the model on the evaluation dataset.\n",
    "\n",
    "    Raises:\n",
    "        ModelTypeNotFoundException: If the provided model_type is neither 'theta' nor 'trig'.\n",
    "\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    running_error = 0.0\n",
    "    count = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, theta, trig) in enumerate(eval_loader):\n",
    "            if model_type == \"theta\":\n",
    "                theta_hat = model(x).squeeze(1)\n",
    "\n",
    "                batch_loss = loss_fn(theta, theta_hat)\n",
    "                batch_error = error_fn(theta, theta_hat)\n",
    "            elif model_type == \"trig\":\n",
    "                trig_hat = model(x)\n",
    "                sin_hat = trig_hat[:, 0]\n",
    "                cos_hat = trig_hat[:, 1]\n",
    "                sin = trig[:, 0]\n",
    "                cos = trig[:, 1]\n",
    "                angle_hat = torch.atan2(sin_hat, cos_hat)\n",
    "                angle = torch.atan2(sin, cos)\n",
    "\n",
    "                batch_loss = loss_fn(trig, trig_hat)\n",
    "                batch_error = error_fn(angle, angle_hat)\n",
    "            else:\n",
    "                raise ModelTypeNotFoundException(\n",
    "                    f\"The model type should be either theta or trig.\"\n",
    "                )\n",
    "\n",
    "            running_loss += batch_loss.item()\n",
    "            running_error += batch_error.item()\n",
    "            count += 1\n",
    "\n",
    "    loss = running_loss / count\n",
    "    error = running_error / count\n",
    "\n",
    "    return loss, error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4852e8-f300-4c47-8d83-1151f6eb2fb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00caecaf1d2ebf545e0444e4132b8b30",
     "grade": false,
     "grade_id": "cell-508be3dc7526b037",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1b.1 - Learn to predict angles directly (9.5 points)\n",
    "We are going to create models that try to predict the link angles $\\hat{\\theta}$ given an image of the robot, so $\\theta \\approx \\hat{\\theta} =M(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8021a8-dc73-4bfe-ac92-f59089ee764b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f66b107dc5c647034896422eb4adee78",
     "grade": false,
     "grade_id": "cell-9161da53b6982892",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tasb 1b.1.1 - Create the CNN model (3.5 points)\n",
    "It's time to create a CNN model. Here, create a PyTorch model class called `CNNTheta()`.\n",
    "* Start with a convolutional layer `torch.nn.Conv2d(...)` with 32 as the output channel number and a kernel size of `3x3`. This convolutional layer should be followed by a ReLU activation function.\n",
    "* Then add an average pooling layer `torch.nn.AvgPool2d(...)` with a pooling kernel size of `2x2`.\n",
    "* Add another convolutional layer with `10` as the output channel number and `3x3` as the kernel size. This convolutional layer is also followed by a ReLU function.\n",
    "* Add another average pooling layer with the kernel size of `2x2`.\n",
    "* Flatten the output of the pooling layer using `torch.nn.Flatten()`.\n",
    "* Add a fully connected layer with `30` as the output channel number, with a ReLU activation function.\n",
    "* Finally, add another fully connected layer without activation function. Remember, the number of output units must match the dimension of the target data, which is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea65aa8-b7e8-44f2-84f5-740ed8d04e34",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e2b51a861463d4cf7fb45f78c43fe72",
     "grade": true,
     "grade_id": "cell-673fceb9cff47145",
     "locked": false,
     "points": 3.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable model parameters:  8071\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TASK 1b.1.1: CREATE THETA MODEL HERE\"\"\"\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "class CNNTheta(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNTheta, self).__init__()\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=10, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(10 * 4 * 4, 30)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(30, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\"\"\"TASK 1b.1.1: END\"\"\"\n",
    "\n",
    "model_theta = CNNTheta()\n",
    "total_params_theta = sum(p.numel() for p in model_theta.parameters())\n",
    "print(\"Total number of trainable model parameters: \", total_params_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f951f-a253-4a21-a618-0ed4fe97d699",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b65910e9ca684feffc50896218b0a67",
     "grade": false,
     "grade_id": "cell-d0d1072d05ddf82c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tasb 1b.1.2 - Train the model (4 points)\n",
    "Now that the model is defined, we can train it using the training dataset. Train the model for 50 epochs with the training data `train_loader`. Tune the learning rate in the optimizer to give the best performance on the validation and test set over the 50 epochs. For each training loop, it would be useful to test the model performance on the validation set to see if it is overfitting the training data or can generalize well. Do this 3 times and use the validation dataset `val_loader` with the function `evaluate_model` to get the validation result. \n",
    "\n",
    "**Hints:** \n",
    "- When we iterate over the training set, we use `(x, theta, _)` or something similar where `_` is used to ignore `trig`.\n",
    "- Reduce the number of epochs and lower the number of runs to `1` while getting your model working and setting the optimal learning rate. \n",
    "- Try varying the learning rate in the `1e-1` to `1e-5` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13327da5-4d1a-4e0c-a730-e80f87893907",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67438125ab90285988e5340e796c4b0f",
     "grade": false,
     "grade_id": "cell-18afa18aa1659afa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the number of runs (i.e., different random seeds)\n",
    "num_runs = 1  # Change to 1 until you get it to work once\n",
    "# number of epochs we train each model for\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c98235-5396-4618-bf07-8264c2fe29ab",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9688eb204787ca1646e7c8a75004ab7b",
     "grade": true,
     "grade_id": "cell-0faed96316d9f061",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a3e69f579146c28c1d14b7474677e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 finished with a test loss of 0.4739 rad^2 and a mean test error of 0.4664 rad.\n"
     ]
    }
   ],
   "source": [
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1b.1.2: TRAIN MODEL HERE\"\"\"\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Starting run {run}\")\n",
    "        # set new random seed\n",
    "        manual_seed(seed=run)\n",
    "        # this code reinitializes the parameters of the model on each loop\n",
    "        model_theta = CNNTheta()\n",
    "\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.SGD(model_theta.parameters(), lr=1e-3)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        '''\n",
    "        #for epoch in range(num_epochs):\n",
    "            #model_theta.train() \n",
    "            #for x, theta, _ in train_loader:\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model_theta(x)\n",
    "                outputs = outputs.sqeeze(dim=1)\n",
    "                loss = loss_fn, outputs, theta.float()\n",
    "                #loss = loss_fn(outputs,outputs)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        '''\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            for x, theta, _ in train_loader:\n",
    "\n",
    "                preds = model_theta(x)\n",
    "                preds = preds.squeeze(dim=1)\n",
    "                \n",
    "                loss = loss_fn(preds, theta.float())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "        run_test_loss, run_test_error = evaluate_model(\n",
    "            model_theta, test_loader, model_type=\"theta\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Run {run} finished with a test loss of {run_test_loss:.4} rad^2 and a mean test error of {run_test_error:.4} rad.\"\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        torch.save(\n",
    "            model_theta.state_dict(),\n",
    "            STATEDICTS_DIR / f\"task_1b_theta_model_run-{run}.pth\",\n",
    "            _use_new_zipfile_serialization=False,\n",
    "        )\n",
    "    \"\"\"TASK 1.1.2: END\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bbf11-d398-45c6-8530-41d350d366d0",
   "metadata": {},
   "source": [
    "Evaluate the final result on the `test_loader` using the function `evaluate_model`. The `model_type` is `theta` in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f99b36-8fd7-4edf-90f7-de6f6362ab69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb4a053eef9acc1b7248aa698783c754",
     "grade": false,
     "grade_id": "cell-734fc06bea8d4bdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error of theta model across runs: 0.4664 += 0.0 rad.\n"
     ]
    }
   ],
   "source": [
    "# evaluate the theta model on the test set\n",
    "test_error_across_runs_theta = np.zeros((num_runs,))\n",
    "for run in range(num_runs):\n",
    "    model_theta.load_state_dict(\n",
    "        torch.load(STATEDICTS_DIR / f\"task_1b_theta_model_run-{run}.pth\")\n",
    "    )\n",
    "    run_test_loss, run_test_error = evaluate_model(\n",
    "        model_theta, test_loader, model_type=\"theta\"\n",
    "    )\n",
    "    test_error_across_runs_theta[run] = run_test_error\n",
    "print(\n",
    "    f\"Prediction error of theta model across runs: {np.mean(test_error_across_runs_theta):.4} += {np.std(test_error_across_runs_theta):.4} rad.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a4085-8fd3-4d75-97d1-61dbfe9e3ade",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0abd9ba17a8a37e477636ce1f66aceca",
     "grade": false,
     "grade_id": "cell-9460d7f8f74cd5ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tasb 1b.1.3 - Analyse model performance (2 points)\n",
    "Evaluate the trained model's accuracy by examining the average error in the link angle prediction. Please analyze the training loss curve: does the training loss decrease step by step? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17da5d-7d20-4475-a19f-a0656816e940",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5a0a3d75d6d4c06040d71e43d4f0c50",
     "grade": true,
     "grade_id": "cell-7e66bf3c2ba23099",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE: The training loss does decrease step by step. We can expect that the error goes down and then plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ea3a6-4f4c-4ca9-a2ee-edd4c6d9ed40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a789cb1cd78b3dff414ca7c2ae04817",
     "grade": false,
     "grade_id": "cell-cf8f9f512c93eb31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We always use a separate test dataset to evaluate a trained model. What is the reason for this? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f23ccb-37a6-4e30-af0e-96f8cc7b8fe1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09234ca54e2dcf141f3e3bbdcf266242",
     "grade": true,
     "grade_id": "cell-98110783648b05de",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE: This is to make sure that the model didnt for example over train to the training data. If using seperate data this will detect this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab22cb-baa1-4ca5-9df1-9196dd9cfca8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a8194c2696729b09d25284f62f78409",
     "grade": false,
     "grade_id": "cell-24f8c2fc51dd1023",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1b.2 - Indirectly predict the angle (10.5 points)\n",
    "We are going to improve the accuracy by pre-processing the target data. Specifically, we will create a model $M_{trig}$ that learns to predict $\\sin(\\theta)$ and $\\cos(\\theta)$ for the pendulum instead of directly predicting $\\theta$. Then, we can use the inverse tangent function to retrieve an estimate of $\\theta$ for both links.\n",
    "\n",
    "*Note:* In practice you would use the `atan2` implementation, as the regular arctangent only covers $[-\\frac{1}{2}\\pi, \\frac{1}{2}\\pi]$} $\\theta=\\arctan(\\frac{\\sin(\\theta)}{\\cos(\\theta)})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065c32d-f32a-43c0-8909-8ff686ddcf1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1508220f81a96c27c65b49a9a8d54f1c",
     "grade": false,
     "grade_id": "cell-fdb86715e0324415",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1b.2.1 - Create the model (3.5 points)\n",
    "Copy the model you created in Task 1.1. Change the number of hidden units of the final layer from 1 to 2. We do so because we now want to predict two outputs ($\\sin(\\theta)$, $\\cos(\\theta)$) for each sample. The total number of the model parameters should increase slightly compared to Task 1.1. _**Optional question:** Why does the number of model parameters increase?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9354f229-7ded-47c8-8506-f90c60016919",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40ddaf9e869460c44e8ceaf6855155d5",
     "grade": true,
     "grade_id": "cell-0fa6b5c9dcd43530",
     "locked": false,
     "points": 3.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of model parameters:  8102\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TASK 1b.2.1: CREATE TRIGONOMETRIC MODEL HERE\"\"\"\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "class CNNTrig(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNTrig, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=10, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(10 * 4 * 4, 30)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(30, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.avgpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\"\"\"TASK 1b.2.1: END\"\"\"\n",
    "\n",
    "model_trig = CNNTrig()\n",
    "total_params_trig = sum(p.numel() for p in model_trig.parameters())\n",
    "print(\"total number of model parameters: \", total_params_trig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e87a4f-d8d0-4134-881a-0a79aa54b85e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42ef35ba68bbb13a7149533dda929eea",
     "grade": false,
     "grade_id": "cell-f3a53b5a2f975228",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1b.2.2 - Train the model (4 points)\n",
    "Now train a model with the same training hyperparameters as in task 1.1 but using the the trigonometric labels. When dealing with the return value of the `train_loader,` be careful to make sure that the output is the $[\\sin(\\theta)$, $\\cos(\\theta)]$ instead of $\\theta$. Tune the learning rate in the optimizer to give the best performance on the validation and test set over the 50 epochs. Do this 3 times like the previous task.\n",
    "\n",
    "**Hint:** \n",
    "- We utilize the third instead of the second item in the tuple returned by the dataloader. Therefore, when we iterate over the training set, we use `(x, _, trig)` where `_` is applied to ignore `theta`. Here, `trig` is a tensor containing $\\sin$ value and $\\cos$ value\n",
    "- Reduce the number of epochs and lower the number of runs to `1` while getting your model working. \n",
    "- Try in the range `1e-1` to `1e-5` for the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e23030e9-be70-4ea0-8621-1e10c687bf38",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea0a78c45020729191bbcf80bee6477c",
     "grade": false,
     "grade_id": "cell-f04acce798995efb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the number of runs (i.e., different random seeds)\n",
    "num_runs = 3  # Change to 1 until you get it to work once\n",
    "# number of epochs we train each model for\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd1861b0-0ff7-456c-b8eb-d697ef26a8d5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce01085d02ea5a2b8e95d39d4e450b73",
     "grade": true,
     "grade_id": "cell-92d614bf44b02cde",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     15\u001b[0m     model_trig\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ignore theta, use trig instead\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_trig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mCNNDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(x)\n\u001b[0;32m---> 46\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marr_0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     47\u001b[0m sin \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(theta)\n\u001b[1;32m     48\u001b[0m cos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(theta)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/npyio.py:444\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic\u001b[38;5;241m.\u001b[39mstartswith(_ZIP_PREFIX) \u001b[38;5;129;01mor\u001b[39;00m magic\u001b[38;5;241m.\u001b[39mstartswith(_ZIP_SUFFIX):\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# zip-file (assume .npz)\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Potentially transfer file ownership to NpzFile\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     stack\u001b[38;5;241m.\u001b[39mpop_all()\n\u001b[0;32m--> 444\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mNpzFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mown_fid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mown_fid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# .npy file\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/npyio.py:190\u001b[0m, in \u001b[0;36mNpzFile.__init__\u001b[0;34m(self, fid, own_fid, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fid, own_fid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    186\u001b[0m              pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    187\u001b[0m              max_header_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39m_MAX_HEADER_SIZE):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# Import is postponed to here since zipfile depends on gzip, an\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# optional component of the so-called standard library.\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     _zip \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_files \u001b[38;5;241m=\u001b[39m _zip\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/npyio.py:103\u001b[0m, in \u001b[0;36mzipfile_factory\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[1;32m    102\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallowZip64\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/zipfile.py:1302\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1302\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_didModify \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/zipfile.py:1365\u001b[0m, in \u001b[0;36mZipFile._RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1363\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1365\u001b[0m     endrec \u001b[38;5;241m=\u001b[39m \u001b[43m_EndRecData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/zipfile.py:302\u001b[0m, in \u001b[0;36m_EndRecData\u001b[0;34m(fpin)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m data \u001b[38;5;241m=\u001b[39m fpin\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m sizeEndCentDir \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     data[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m==\u001b[39m stringEndArchive \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\000\u001b[39;00m\u001b[38;5;130;01m\\000\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# the signature is correct and there's no comment, unpack structure\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     endrec \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(structEndArchive, data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not AUTOGRADING:\n",
    "    \"\"\"TASK 1b.2.2: TRAIN TRIGONOMETRIC MODEL HERE\"\"\"\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Starting run {run}\")\n",
    "        # set new random seed\n",
    "        manual_seed(seed=run)\n",
    "        # this code reinitializes the parameters of the model on each loop\n",
    "        model_trig = CNNTrig()\n",
    "\n",
    "        # define the optimizer\n",
    "        optimizer = torch.optim.SGD(model_trig.parameters(), lr=1e-2)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        for epoch in range(num_epochs):\n",
    "            model_trig.train()  # Set the model to training mode\n",
    "            for x, _, trig in train_loader:  # Ignore theta, use trig instead\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model_trig(x)\n",
    "                loss = loss_fn(outputs, trig)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        run_test_loss, run_test_error = evaluate_model(\n",
    "            model_trig, test_loader, model_type=\"trig\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Run {run} finished with a test loss of {run_test_loss:.4} and a mean test error of {run_test_error:.4} rad.\"\n",
    "        )\n",
    "\n",
    "        # save the model\n",
    "        torch.save(\n",
    "            model_trig.state_dict(),\n",
    "            STATEDICTS_DIR / f\"task_1b_trig_model_run-{run}.pth\",\n",
    "            _use_new_zipfile_serialization=False,\n",
    "        )\n",
    "    \"\"\"TASK 1b.2.2: END\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2510",
   "metadata": {},
   "source": [
    "Evaluate the final result on the `test_loader` using the function `evaluate_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960c0a6-b525-44d9-9886-97e100d7970b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f25b115cab9d168877c02874cf1852e",
     "grade": false,
     "grade_id": "cell-47ab7326c7589e4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction error of trig model across runs: 0.02956 += 0.009744 rad.\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trig model on the test set\n",
    "test_error_across_runs_trig = np.zeros((num_runs,))\n",
    "for run in range(num_runs):\n",
    "    model_trig.load_state_dict(\n",
    "        torch.load(STATEDICTS_DIR / f\"task_1b_trig_model_run-{run}.pth\")\n",
    "    )\n",
    "    run_test_loss, run_test_error = evaluate_model(\n",
    "        model_trig, test_loader, model_type=\"trig\"\n",
    "    )\n",
    "    test_error_across_runs_trig[run] = run_test_error\n",
    "print(\n",
    "    f\"Prediction error of trig model across runs: {np.mean(test_error_across_runs_trig):.4} += {np.std(test_error_across_runs_trig):.4} rad.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36a088-36e8-4f7b-982c-7b48c7bc5fd2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01be8a369ef830c2061240a3243297fb",
     "grade": false,
     "grade_id": "cell-8c8760e8514e9fbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1b.2.3 - Analyse model performance (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3be0d3-ee31-421a-87b2-1e380f25d97a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbcff7117462baee761aab716eefec83",
     "grade": false,
     "grade_id": "cell-f3f9f302951f0706",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Analyse the model's training loss. Does your training loss decrease step by step? Do you find some magnitude difference between the training loss of $M_{trig}$ with the previous one of $M_θ$? Can you explain why they are different? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcc579-4edf-4cda-8abe-7ee644349854",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a1808414d6d10b58a139a02791a5278",
     "grade": true,
     "grade_id": "cell-f9f43202ce5d05aa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d962f-3818-453e-9ac5-0b68672a9ae0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d07cb15513aedd98f1abf9dda736081f",
     "grade": false,
     "grade_id": "cell-7860d8b0c325435f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Compare the prediction estimates for $M_{trig}$ with the plot for $M_θ$. Why does indirectly predicting the angle improve the prediction accuracy? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bebf84-5ed5-4305-a5e1-464272771680",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb6797f63a583b32d26db30a0d628ede",
     "grade": false,
     "grade_id": "cell-075334b7096b05f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Why is it not sufficient to predict only sin(θ) and use its inverse θ = arcsin(sin(θ)) to get an estimate of the angle? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe63d97-7921-4534-8008-b105e3520ebf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ac17047ececf4ebade6c286795823e4",
     "grade": true,
     "grade_id": "cell-7425567558dc530e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52e93b-ed7c-4bd4-928b-7ab7b48ebea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
